2024-01-28 15:07:44,273	44k	INFO	{'train': {'log_interval': 250, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0003, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 18, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'yousa': 0}, 'model_dir': './logs/44k'}
2024-01-28 15:07:44,273	44k	WARNING	/root/autodl-tmp/so-vits-svc4 is not a git repository, therefore hash value comparison will be ignored.
2024-01-28 15:07:46,919	44k	INFO	Loaded checkpoint './logs/44k/G_0.pth' (iteration 1)
2024-01-28 15:07:47,274	44k	INFO	Loaded checkpoint './logs/44k/D_0.pth' (iteration 1)
2024-01-28 15:08:45,952	44k	INFO	Train Epoch: 1 [0%]
2024-01-28 15:08:45,953	44k	INFO	Losses: [2.4071338176727295, 3.0069539546966553, 12.67491626739502, 37.54408264160156, 2.9463412761688232], step: 0, lr: 0.0002
2024-01-28 15:08:59,219	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2024-01-28 15:09:00,730	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2024-01-28 15:09:32,885	44k	INFO	====> Epoch: 1, cost 108.61 s
2024-01-28 15:10:57,464	44k	INFO	====> Epoch: 2, cost 84.58 s
2024-01-28 15:12:20,036	44k	INFO	====> Epoch: 3, cost 82.57 s
2024-01-28 15:13:41,711	44k	INFO	====> Epoch: 4, cost 81.68 s
2024-01-28 15:15:04,508	44k	INFO	====> Epoch: 5, cost 82.80 s
2024-01-28 15:16:16,922	44k	INFO	Train Epoch: 6 [68%]
2024-01-28 15:16:16,922	44k	INFO	Losses: [2.6124672889709473, 2.445340156555176, 10.947479248046875, 28.38551139831543, 1.4522565603256226], step: 250, lr: 0.00019987503124609398
2024-01-28 15:16:26,848	44k	INFO	====> Epoch: 6, cost 82.34 s
2024-01-28 15:17:48,846	44k	INFO	====> Epoch: 7, cost 82.00 s
2024-01-28 15:19:11,561	44k	INFO	====> Epoch: 8, cost 82.72 s
2024-01-28 15:20:34,268	44k	INFO	====> Epoch: 9, cost 82.71 s
2024-01-28 15:21:55,915	44k	INFO	====> Epoch: 10, cost 81.65 s
2024-01-28 15:23:17,942	44k	INFO	====> Epoch: 11, cost 82.03 s
2024-01-28 15:24:21,248	44k	INFO	Train Epoch: 12 [36%]
2024-01-28 15:24:21,248	44k	INFO	Losses: [2.3702242374420166, 2.4683098793029785, 12.201314926147461, 27.670032501220703, 1.4965972900390625], step: 500, lr: 0.00019972517181056292
2024-01-28 15:24:40,778	44k	INFO	====> Epoch: 12, cost 82.84 s
2024-01-28 15:26:02,816	44k	INFO	====> Epoch: 13, cost 82.04 s
2024-01-28 15:27:25,540	44k	INFO	====> Epoch: 14, cost 82.72 s
2024-01-28 15:28:47,956	44k	INFO	====> Epoch: 15, cost 82.42 s
2024-01-28 15:30:10,454	44k	INFO	====> Epoch: 16, cost 82.50 s
2024-01-28 15:31:32,991	44k	INFO	====> Epoch: 17, cost 82.54 s
2024-01-28 15:32:26,936	44k	INFO	Train Epoch: 18 [5%]
2024-01-28 15:32:26,936	44k	INFO	Losses: [2.411832332611084, 2.781433582305908, 11.549125671386719, 27.775083541870117, 1.4128755331039429], step: 750, lr: 0.00019957542473449108
2024-01-28 15:32:55,777	44k	INFO	====> Epoch: 18, cost 82.79 s
2024-01-28 15:34:18,485	44k	INFO	====> Epoch: 19, cost 82.71 s
2024-01-28 15:35:40,378	44k	INFO	====> Epoch: 20, cost 81.89 s
2024-01-28 15:37:03,121	44k	INFO	====> Epoch: 21, cost 82.74 s
2024-01-28 15:38:24,771	44k	INFO	====> Epoch: 22, cost 81.65 s
2024-01-28 15:39:39,032	44k	INFO	Train Epoch: 23 [73%]
2024-01-28 15:39:39,033	44k	INFO	Losses: [2.2906408309936523, 3.050128698348999, 11.935342788696289, 27.172346115112305, 1.4475679397583008], step: 1000, lr: 0.00019945072127379438
2024-01-28 15:39:51,550	44k	INFO	Saving model and optimizer state at iteration 23 to ./logs/44k/G_1000.pth
2024-01-28 15:39:52,746	44k	INFO	Saving model and optimizer state at iteration 23 to ./logs/44k/D_1000.pth
2024-01-28 15:40:01,403	44k	INFO	====> Epoch: 23, cost 96.63 s
2024-01-28 15:41:23,985	44k	INFO	====> Epoch: 24, cost 82.58 s
2024-01-28 15:42:46,678	44k	INFO	====> Epoch: 25, cost 82.69 s
2024-01-28 15:44:08,459	44k	INFO	====> Epoch: 26, cost 81.78 s
2024-01-28 15:45:31,024	44k	INFO	====> Epoch: 27, cost 82.57 s
2024-01-28 15:46:54,501	44k	INFO	====> Epoch: 28, cost 83.48 s
2024-01-28 15:47:59,454	44k	INFO	Train Epoch: 29 [41%]
2024-01-28 15:47:59,455	44k	INFO	Losses: [2.3663887977600098, 2.615967035293579, 12.727498054504395, 29.439682006835938, 1.4581973552703857], step: 1250, lr: 0.0001993011799713115
2024-01-28 15:48:17,645	44k	INFO	====> Epoch: 29, cost 83.14 s
2024-01-28 15:49:39,906	44k	INFO	====> Epoch: 30, cost 82.26 s
2024-01-28 15:51:04,108	44k	INFO	====> Epoch: 31, cost 84.20 s
2024-01-28 15:52:28,283	44k	INFO	====> Epoch: 32, cost 84.17 s
2024-01-28 15:53:52,549	44k	INFO	====> Epoch: 33, cost 84.27 s
2024-01-28 15:55:15,903	44k	INFO	====> Epoch: 34, cost 83.35 s
2024-01-28 15:56:12,467	44k	INFO	Train Epoch: 35 [9%]
2024-01-28 15:56:12,468	44k	INFO	Losses: [2.3740170001983643, 2.4925777912139893, 14.490387916564941, 27.7060489654541, 1.2586225271224976], step: 1500, lr: 0.00019915175078976256
2024-01-28 15:56:40,359	44k	INFO	====> Epoch: 35, cost 84.46 s
2024-01-28 15:58:05,420	44k	INFO	====> Epoch: 36, cost 85.06 s
2024-01-28 15:59:28,514	44k	INFO	====> Epoch: 37, cost 83.09 s
2024-01-28 16:00:53,228	44k	INFO	====> Epoch: 38, cost 84.71 s
2024-01-28 16:02:15,508	44k	INFO	====> Epoch: 39, cost 82.28 s
2024-01-28 16:03:31,270	44k	INFO	Train Epoch: 40 [77%]
2024-01-28 16:03:31,271	44k	INFO	Losses: [2.4898312091827393, 2.7611494064331055, 9.93452262878418, 26.439966201782227, 1.3830102682113647], step: 1750, lr: 0.0001990273120590905
2024-01-28 16:03:38,713	44k	INFO	====> Epoch: 40, cost 83.20 s
2024-01-28 16:05:01,183	44k	INFO	====> Epoch: 41, cost 82.47 s
2024-01-28 16:06:23,734	44k	INFO	====> Epoch: 42, cost 82.55 s
2024-01-28 16:07:46,504	44k	INFO	====> Epoch: 43, cost 82.77 s
2024-01-28 16:09:08,723	44k	INFO	====> Epoch: 44, cost 82.22 s
2024-01-28 16:10:31,517	44k	INFO	====> Epoch: 45, cost 82.79 s
2024-01-28 16:11:39,226	44k	INFO	Train Epoch: 46 [45%]
2024-01-28 16:11:39,227	44k	INFO	Losses: [2.356557607650757, 2.386847734451294, 11.625955581665039, 25.510007858276367, 1.0534545183181763], step: 2000, lr: 0.00019887808821429862
2024-01-28 16:11:51,565	44k	INFO	Saving model and optimizer state at iteration 46 to ./logs/44k/G_2000.pth
2024-01-28 16:11:52,909	44k	INFO	Saving model and optimizer state at iteration 46 to ./logs/44k/D_2000.pth
2024-01-28 16:12:09,710	44k	INFO	====> Epoch: 46, cost 98.19 s
2024-01-28 16:13:32,498	44k	INFO	====> Epoch: 47, cost 82.79 s
2024-01-28 16:14:54,910	44k	INFO	====> Epoch: 48, cost 82.41 s
2024-01-28 16:16:17,996	44k	INFO	====> Epoch: 49, cost 83.09 s
2024-01-28 16:17:40,518	44k	INFO	====> Epoch: 50, cost 82.52 s
2024-01-28 16:19:03,194	44k	INFO	====> Epoch: 51, cost 82.68 s
2024-01-28 16:19:59,361	44k	INFO	Train Epoch: 52 [14%]
2024-01-28 16:19:59,363	44k	INFO	Losses: [2.2203333377838135, 2.7485601902008057, 12.329662322998047, 26.65388298034668, 1.229658603668213], step: 2250, lr: 0.00019872897625242182
2024-01-28 16:20:25,669	44k	INFO	====> Epoch: 52, cost 82.48 s
2024-01-28 16:22:00,655	44k	INFO	====> Epoch: 53, cost 94.99 s
2024-01-28 16:23:23,272	44k	INFO	====> Epoch: 54, cost 82.62 s
2024-01-28 16:24:46,577	44k	INFO	====> Epoch: 55, cost 83.31 s
2024-01-28 16:26:09,731	44k	INFO	====> Epoch: 56, cost 83.15 s
2024-01-28 16:27:28,007	44k	INFO	Train Epoch: 57 [82%]
2024-01-28 16:27:28,007	44k	INFO	Losses: [2.2244715690612793, 2.6330461502075195, 12.491559028625488, 25.971961975097656, 1.2321014404296875], step: 2500, lr: 0.00019860480168978534
2024-01-28 16:27:34,169	44k	INFO	====> Epoch: 57, cost 84.44 s
2024-01-28 16:28:58,214	44k	INFO	====> Epoch: 58, cost 84.04 s
2024-01-28 16:30:21,300	44k	INFO	====> Epoch: 59, cost 83.09 s
2024-01-28 16:31:44,604	44k	INFO	====> Epoch: 60, cost 83.30 s
2024-01-28 16:33:08,133	44k	INFO	====> Epoch: 61, cost 83.53 s
2024-01-28 16:34:31,904	44k	INFO	====> Epoch: 62, cost 83.77 s
2024-01-28 16:35:40,723	44k	INFO	Train Epoch: 63 [50%]
2024-01-28 16:35:40,724	44k	INFO	Losses: [2.4532973766326904, 2.4028241634368896, 10.061091423034668, 26.22584342956543, 1.2903202772140503], step: 2750, lr: 0.00019845589462876104
2024-01-28 16:35:56,387	44k	INFO	====> Epoch: 63, cost 84.48 s
2024-01-28 16:37:20,197	44k	INFO	====> Epoch: 64, cost 83.81 s
2024-01-28 16:38:43,983	44k	INFO	====> Epoch: 65, cost 83.79 s
2024-01-28 16:40:07,450	44k	INFO	====> Epoch: 66, cost 83.47 s
2024-01-28 16:41:29,501	44k	INFO	====> Epoch: 67, cost 82.05 s
2024-01-28 16:42:51,741	44k	INFO	====> Epoch: 68, cost 82.24 s
2024-01-28 16:43:49,714	44k	INFO	Train Epoch: 69 [18%]
2024-01-28 16:43:49,715	44k	INFO	Losses: [2.238333225250244, 2.569279670715332, 13.028765678405762, 26.736173629760742, 1.1764655113220215], step: 3000, lr: 0.0001983070992131383
2024-01-28 16:44:02,616	44k	INFO	Saving model and optimizer state at iteration 69 to ./logs/44k/G_3000.pth
2024-01-28 16:44:03,922	44k	INFO	Saving model and optimizer state at iteration 69 to ./logs/44k/D_3000.pth
2024-01-28 16:44:28,773	44k	INFO	====> Epoch: 69, cost 97.03 s
2024-01-28 16:45:51,344	44k	INFO	====> Epoch: 70, cost 82.57 s
2024-01-28 16:47:15,222	44k	INFO	====> Epoch: 71, cost 83.88 s
2024-01-28 16:48:37,410	44k	INFO	====> Epoch: 72, cost 82.19 s
2024-01-28 16:50:00,101	44k	INFO	====> Epoch: 73, cost 82.69 s
2024-01-28 16:51:22,138	44k	INFO	Train Epoch: 74 [86%]
2024-01-28 16:51:22,138	44k	INFO	Losses: [2.2314319610595703, 2.7963783740997314, 12.733495712280273, 27.905729293823242, 1.3539854288101196], step: 3250, lr: 0.00019818318825774137
2024-01-28 16:51:26,656	44k	INFO	====> Epoch: 74, cost 86.55 s
2024-01-28 16:52:48,978	44k	INFO	====> Epoch: 75, cost 82.32 s
2024-01-28 16:54:11,500	44k	INFO	====> Epoch: 76, cost 82.52 s
2024-01-28 16:55:34,991	44k	INFO	====> Epoch: 77, cost 83.49 s
2024-01-28 16:56:58,918	44k	INFO	====> Epoch: 78, cost 83.93 s
2024-01-28 16:58:21,785	44k	INFO	====> Epoch: 79, cost 82.87 s
2024-01-28 16:59:30,975	44k	INFO	Train Epoch: 80 [55%]
2024-01-28 16:59:30,975	44k	INFO	Losses: [2.2449824810028076, 2.4789819717407227, 13.058106422424316, 27.188350677490234, 1.2751227617263794], step: 3500, lr: 0.00019803459730799195
2024-01-28 16:59:44,965	44k	INFO	====> Epoch: 80, cost 83.18 s
2024-01-28 17:01:07,552	44k	INFO	====> Epoch: 81, cost 82.59 s
2024-01-28 17:02:30,051	44k	INFO	====> Epoch: 82, cost 82.50 s
2024-01-28 17:03:56,217	44k	INFO	====> Epoch: 83, cost 86.17 s
2024-01-28 17:05:18,866	44k	INFO	====> Epoch: 84, cost 82.65 s
2024-01-28 17:06:41,605	44k	INFO	====> Epoch: 85, cost 82.74 s
2024-01-28 17:07:41,832	44k	INFO	Train Epoch: 86 [23%]
2024-01-28 17:07:41,833	44k	INFO	Losses: [2.3315272331237793, 2.6807973384857178, 11.309173583984375, 25.90644073486328, 1.1961792707443237], step: 3750, lr: 0.00019788611776663464
2024-01-28 17:08:05,653	44k	INFO	====> Epoch: 86, cost 84.05 s
2024-01-28 17:09:28,700	44k	INFO	====> Epoch: 87, cost 83.05 s
2024-01-28 17:10:50,878	44k	INFO	====> Epoch: 88, cost 82.18 s
2024-01-28 17:12:14,154	44k	INFO	====> Epoch: 89, cost 83.28 s
2024-01-28 17:13:36,938	44k	INFO	====> Epoch: 90, cost 82.78 s
2024-01-28 17:14:57,152	44k	INFO	Train Epoch: 91 [91%]
2024-01-28 17:14:57,153	44k	INFO	Losses: [2.2231175899505615, 2.6944916248321533, 10.256726264953613, 25.72042465209961, 1.30376398563385], step: 4000, lr: 0.00019776246985887165
2024-01-28 17:15:09,643	44k	INFO	Saving model and optimizer state at iteration 91 to ./logs/44k/G_4000.pth
2024-01-28 17:15:10,916	44k	INFO	Saving model and optimizer state at iteration 91 to ./logs/44k/D_4000.pth
2024-01-28 17:15:14,051	44k	INFO	====> Epoch: 91, cost 97.11 s
2024-01-28 17:16:36,796	44k	INFO	====> Epoch: 92, cost 82.74 s
2024-01-28 17:17:59,394	44k	INFO	====> Epoch: 93, cost 82.60 s
2024-01-28 17:19:22,422	44k	INFO	====> Epoch: 94, cost 83.03 s
2024-01-28 17:20:44,974	44k	INFO	====> Epoch: 95, cost 82.55 s
2024-01-28 17:22:07,583	44k	INFO	====> Epoch: 96, cost 82.61 s
2024-01-28 17:23:18,457	44k	INFO	Train Epoch: 97 [59%]
2024-01-28 17:23:18,457	44k	INFO	Losses: [2.0292108058929443, 2.8025922775268555, 12.558484077453613, 26.671234130859375, 1.0769734382629395], step: 4250, lr: 0.00019761419434933197
2024-01-28 17:23:31,070	44k	INFO	====> Epoch: 97, cost 83.49 s
2024-01-28 17:24:54,620	44k	INFO	====> Epoch: 98, cost 83.55 s
2024-01-28 17:26:17,996	44k	INFO	====> Epoch: 99, cost 83.38 s
2024-01-28 17:27:40,433	44k	INFO	====> Epoch: 100, cost 82.44 s
2024-01-28 17:29:03,021	44k	INFO	====> Epoch: 101, cost 82.59 s
2024-01-28 17:30:25,924	44k	INFO	====> Epoch: 102, cost 82.90 s
2024-01-28 17:31:27,428	44k	INFO	Train Epoch: 103 [27%]
2024-01-28 17:31:27,429	44k	INFO	Losses: [2.4684557914733887, 2.569866180419922, 11.570061683654785, 24.66657829284668, 1.1398836374282837], step: 4500, lr: 0.00019746603001167813
2024-01-28 17:31:49,650	44k	INFO	====> Epoch: 103, cost 83.73 s
2024-01-28 17:33:12,402	44k	INFO	====> Epoch: 104, cost 82.75 s
2024-01-28 17:34:35,584	44k	INFO	====> Epoch: 105, cost 83.18 s
2024-01-28 17:35:59,056	44k	INFO	====> Epoch: 106, cost 83.47 s
2024-01-28 17:37:23,151	44k	INFO	====> Epoch: 107, cost 84.09 s
2024-01-28 17:38:44,838	44k	INFO	Train Epoch: 108 [95%]
2024-01-28 17:38:44,839	44k	INFO	Losses: [2.261751174926758, 2.374675750732422, 12.938247680664062, 25.666213989257812, 1.054665446281433], step: 4750, lr: 0.00019734264459313146
2024-01-28 17:38:46,610	44k	INFO	====> Epoch: 108, cost 83.46 s
2024-01-28 17:40:08,471	44k	INFO	====> Epoch: 109, cost 81.86 s
2024-01-28 17:41:31,179	44k	INFO	====> Epoch: 110, cost 82.71 s
2024-01-28 17:42:56,236	44k	INFO	====> Epoch: 111, cost 85.06 s
2024-01-28 17:44:21,903	44k	INFO	====> Epoch: 112, cost 85.67 s
2024-01-28 17:45:44,385	44k	INFO	====> Epoch: 113, cost 82.48 s
2024-01-28 17:46:56,584	44k	INFO	Train Epoch: 114 [64%]
2024-01-28 17:46:56,585	44k	INFO	Losses: [2.325913667678833, 2.40157413482666, 11.149168968200684, 25.641891479492188, 1.0199447870254517], step: 5000, lr: 0.0001971946838541609
2024-01-28 17:47:09,215	44k	INFO	Saving model and optimizer state at iteration 114 to ./logs/44k/G_5000.pth
2024-01-28 17:47:10,508	44k	INFO	Saving model and optimizer state at iteration 114 to ./logs/44k/D_5000.pth
2024-01-28 17:47:21,832	44k	INFO	====> Epoch: 114, cost 97.45 s
2024-01-28 17:48:43,995	44k	INFO	====> Epoch: 115, cost 82.16 s
2024-01-28 17:50:07,304	44k	INFO	====> Epoch: 116, cost 83.31 s
2024-01-28 17:51:31,009	44k	INFO	====> Epoch: 117, cost 83.70 s
2024-01-28 17:52:54,011	44k	INFO	====> Epoch: 118, cost 83.00 s
2024-01-28 17:54:17,538	44k	INFO	====> Epoch: 119, cost 83.53 s
2024-01-28 17:55:20,090	44k	INFO	Train Epoch: 120 [32%]
2024-01-28 17:55:20,091	44k	INFO	Losses: [2.270537853240967, 2.8682193756103516, 11.869479179382324, 26.26702880859375, 1.1405106782913208], step: 5250, lr: 0.000197046834051072
2024-01-28 17:55:40,834	44k	INFO	====> Epoch: 120, cost 83.29 s
2024-01-28 17:57:03,859	44k	INFO	====> Epoch: 121, cost 83.03 s
2024-01-28 17:58:26,442	44k	INFO	====> Epoch: 122, cost 82.58 s
2024-01-28 17:59:49,449	44k	INFO	====> Epoch: 123, cost 83.01 s
2024-01-28 18:01:12,624	44k	INFO	====> Epoch: 124, cost 83.17 s
2024-01-28 18:02:35,350	44k	INFO	====> Epoch: 125, cost 82.73 s
2024-01-28 18:03:28,459	44k	INFO	Train Epoch: 126 [0%]
2024-01-28 18:03:28,460	44k	INFO	Losses: [2.0836381912231445, 2.7098701000213623, 12.53842544555664, 26.392072677612305, 1.1585938930511475], step: 5500, lr: 0.000196899095100689
2024-01-28 18:03:58,678	44k	INFO	====> Epoch: 126, cost 83.33 s
2024-01-28 18:05:22,298	44k	INFO	====> Epoch: 127, cost 83.62 s
2024-01-28 18:06:44,739	44k	INFO	====> Epoch: 128, cost 82.44 s
2024-01-28 18:08:07,793	44k	INFO	====> Epoch: 129, cost 83.05 s
2024-01-28 18:09:31,751	44k	INFO	====> Epoch: 130, cost 83.96 s
2024-01-28 18:10:46,527	44k	INFO	Train Epoch: 131 [68%]
2024-01-28 18:10:46,528	44k	INFO	Losses: [2.4065053462982178, 2.603675127029419, 12.384806632995605, 26.278894424438477, 1.2008638381958008], step: 5750, lr: 0.00019677606392788917
2024-01-28 18:10:56,501	44k	INFO	====> Epoch: 131, cost 84.75 s
2024-01-28 18:12:19,886	44k	INFO	====> Epoch: 132, cost 83.38 s
2024-01-28 18:13:43,189	44k	INFO	====> Epoch: 133, cost 83.30 s
2024-01-28 18:15:05,682	44k	INFO	====> Epoch: 134, cost 82.49 s
2024-01-28 18:16:28,374	44k	INFO	====> Epoch: 135, cost 82.69 s
2024-01-28 18:17:51,394	44k	INFO	====> Epoch: 136, cost 83.02 s
2024-01-28 18:18:55,279	44k	INFO	Train Epoch: 137 [36%]
2024-01-28 18:18:55,279	44k	INFO	Losses: [2.2904317378997803, 2.6048569679260254, 10.631484031677246, 25.772811889648438, 1.0317661762237549], step: 6000, lr: 0.00019662852799164733
2024-01-28 18:19:07,951	44k	INFO	Saving model and optimizer state at iteration 137 to ./logs/44k/G_6000.pth
2024-01-28 18:19:09,207	44k	INFO	Saving model and optimizer state at iteration 137 to ./logs/44k/D_6000.pth
2024-01-28 18:19:28,611	44k	INFO	====> Epoch: 137, cost 97.22 s
2024-01-28 18:20:52,169	44k	INFO	====> Epoch: 138, cost 83.56 s
2024-01-28 18:22:15,420	44k	INFO	====> Epoch: 139, cost 83.25 s
2024-01-28 18:23:38,994	44k	INFO	====> Epoch: 140, cost 83.57 s
2024-01-28 18:25:02,652	44k	INFO	====> Epoch: 141, cost 83.66 s
2024-01-28 18:26:26,015	44k	INFO	====> Epoch: 142, cost 83.36 s
2024-01-28 18:27:20,777	44k	INFO	Train Epoch: 143 [5%]
2024-01-28 18:27:20,777	44k	INFO	Losses: [2.452627658843994, 2.964834213256836, 10.483119010925293, 24.67078399658203, 1.0256950855255127], step: 6250, lr: 0.0001964811026727847
2024-01-28 18:27:49,750	44k	INFO	====> Epoch: 143, cost 83.74 s
2024-01-28 18:29:12,521	44k	INFO	====> Epoch: 144, cost 82.77 s
2024-01-28 18:30:35,852	44k	INFO	====> Epoch: 145, cost 83.33 s
2024-01-28 18:31:58,504	44k	INFO	====> Epoch: 146, cost 82.65 s
2024-01-28 18:33:20,863	44k	INFO	====> Epoch: 147, cost 82.36 s
2024-01-28 18:34:36,024	44k	INFO	Train Epoch: 148 [73%]
2024-01-28 18:34:36,025	44k	INFO	Losses: [2.2524220943450928, 2.824193239212036, 12.710430145263672, 25.5477294921875, 1.0971200466156006], step: 6500, lr: 0.00019635833267994917
2024-01-28 18:34:44,599	44k	INFO	====> Epoch: 148, cost 83.74 s
2024-01-28 18:36:07,659	44k	INFO	====> Epoch: 149, cost 83.06 s
2024-01-28 18:37:31,082	44k	INFO	====> Epoch: 150, cost 83.42 s
2024-01-28 18:38:54,305	44k	INFO	====> Epoch: 151, cost 83.22 s
2024-01-28 18:40:17,395	44k	INFO	====> Epoch: 152, cost 83.09 s
2024-01-28 18:41:40,014	44k	INFO	====> Epoch: 153, cost 82.62 s
2024-01-28 18:42:46,258	44k	INFO	Train Epoch: 154 [41%]
2024-01-28 18:42:46,259	44k	INFO	Losses: [2.0784428119659424, 2.758110284805298, 13.698753356933594, 25.493621826171875, 0.9951769709587097], step: 6750, lr: 0.00019621110994425385
2024-01-28 18:43:04,455	44k	INFO	====> Epoch: 154, cost 84.44 s
2024-01-28 18:44:27,087	44k	INFO	====> Epoch: 155, cost 82.63 s
2024-01-28 18:45:50,520	44k	INFO	====> Epoch: 156, cost 83.43 s
2024-01-28 18:47:13,139	44k	INFO	====> Epoch: 157, cost 82.62 s
2024-01-28 18:48:36,303	44k	INFO	====> Epoch: 158, cost 83.16 s
2024-01-28 18:49:59,314	44k	INFO	====> Epoch: 159, cost 83.01 s
2024-01-28 18:50:55,478	44k	INFO	Train Epoch: 160 [9%]
2024-01-28 18:50:55,479	44k	INFO	Losses: [2.113795757293701, 2.6011064052581787, 13.377708435058594, 25.984054565429688, 1.1187877655029297], step: 7000, lr: 0.00019606399759111075
2024-01-28 18:51:07,735	44k	INFO	Saving model and optimizer state at iteration 160 to ./logs/44k/G_7000.pth
2024-01-28 18:51:09,162	44k	INFO	Saving model and optimizer state at iteration 160 to ./logs/44k/D_7000.pth
2024-01-28 18:51:36,802	44k	INFO	====> Epoch: 160, cost 97.49 s
2024-01-28 18:52:59,394	44k	INFO	====> Epoch: 161, cost 82.59 s
2024-01-28 18:54:22,260	44k	INFO	====> Epoch: 162, cost 82.87 s
2024-01-28 18:55:44,628	44k	INFO	====> Epoch: 163, cost 82.37 s
2024-01-28 18:57:07,154	44k	INFO	====> Epoch: 164, cost 82.53 s
2024-01-28 18:58:23,450	44k	INFO	Train Epoch: 165 [77%]
2024-01-28 18:58:23,450	44k	INFO	Losses: [2.283859968185425, 2.6769049167633057, 11.586304664611816, 26.24205207824707, 1.1462509632110596], step: 7250, lr: 0.00019594148822378677
2024-01-28 18:58:30,705	44k	INFO	====> Epoch: 165, cost 83.55 s
2024-01-28 18:59:53,381	44k	INFO	====> Epoch: 166, cost 82.68 s
2024-01-28 19:01:15,603	44k	INFO	====> Epoch: 167, cost 82.22 s
2024-01-28 19:02:37,784	44k	INFO	====> Epoch: 168, cost 82.18 s
2024-01-28 19:03:59,732	44k	INFO	====> Epoch: 169, cost 81.95 s
2024-01-28 19:05:22,676	44k	INFO	====> Epoch: 170, cost 82.94 s
2024-01-28 19:06:29,680	44k	INFO	Train Epoch: 171 [45%]
2024-01-28 19:06:29,682	44k	INFO	Losses: [2.3189172744750977, 2.610161304473877, 10.607016563415527, 26.052837371826172, 1.0333919525146484], step: 7500, lr: 0.00019579457802375197
2024-01-28 19:06:46,516	44k	INFO	====> Epoch: 171, cost 83.84 s
2024-01-28 19:08:09,466	44k	INFO	====> Epoch: 172, cost 82.95 s
2024-01-28 19:09:32,599	44k	INFO	====> Epoch: 173, cost 83.13 s
2024-01-28 19:10:55,845	44k	INFO	====> Epoch: 174, cost 83.25 s
2024-01-28 19:12:19,153	44k	INFO	====> Epoch: 175, cost 83.31 s
2024-01-28 19:13:42,346	44k	INFO	====> Epoch: 176, cost 83.19 s
2024-01-28 19:14:39,474	44k	INFO	Train Epoch: 177 [14%]
2024-01-28 19:14:39,475	44k	INFO	Losses: [2.2959649562835693, 2.894390344619751, 12.761917114257812, 26.6910400390625, 1.120154857635498], step: 7750, lr: 0.00019564777797194082
2024-01-28 19:15:05,748	44k	INFO	====> Epoch: 177, cost 83.40 s
2024-01-28 19:16:28,328	44k	INFO	====> Epoch: 178, cost 82.58 s
2024-01-28 19:17:52,033	44k	INFO	====> Epoch: 179, cost 83.71 s
2024-01-28 19:19:16,022	44k	INFO	====> Epoch: 180, cost 83.99 s
2024-01-28 19:20:39,069	44k	INFO	====> Epoch: 181, cost 83.05 s
2024-01-28 19:21:56,804	44k	INFO	Train Epoch: 182 [82%]
2024-01-28 19:21:56,805	44k	INFO	Losses: [2.267634153366089, 2.685716390609741, 11.826388359069824, 26.27960777282715, 1.1284350156784058], step: 8000, lr: 0.00019552552867685262
2024-01-28 19:22:09,787	44k	INFO	Saving model and optimizer state at iteration 182 to ./logs/44k/G_8000.pth
2024-01-28 19:22:11,029	44k	INFO	Saving model and optimizer state at iteration 182 to ./logs/44k/D_8000.pth
2024-01-28 19:22:16,892	44k	INFO	====> Epoch: 182, cost 97.82 s
2024-01-28 19:23:39,951	44k	INFO	====> Epoch: 183, cost 83.06 s
2024-01-28 19:25:03,443	44k	INFO	====> Epoch: 184, cost 83.49 s
2024-01-28 19:26:26,906	44k	INFO	====> Epoch: 185, cost 83.46 s
2024-01-28 19:27:50,416	44k	INFO	====> Epoch: 186, cost 83.51 s
2024-01-28 19:29:13,666	44k	INFO	====> Epoch: 187, cost 83.25 s
2024-01-28 19:30:21,980	44k	INFO	Train Epoch: 188 [50%]
2024-01-28 19:30:21,981	44k	INFO	Losses: [2.347456932067871, 2.598653793334961, 11.18051528930664, 25.10934066772461, 0.8762698769569397], step: 8250, lr: 0.0001953789303490037
2024-01-28 19:30:37,274	44k	INFO	====> Epoch: 188, cost 83.61 s
2024-01-28 19:32:00,453	44k	INFO	====> Epoch: 189, cost 83.18 s
2024-01-28 19:33:23,675	44k	INFO	====> Epoch: 190, cost 83.22 s
2024-01-28 19:34:47,265	44k	INFO	====> Epoch: 191, cost 83.59 s
2024-01-28 19:36:10,093	44k	INFO	====> Epoch: 192, cost 82.83 s
2024-01-28 19:37:33,889	44k	INFO	====> Epoch: 193, cost 83.80 s
2024-01-28 19:38:32,843	44k	INFO	Train Epoch: 194 [18%]
2024-01-28 19:38:32,844	44k	INFO	Losses: [2.470794677734375, 2.654082775115967, 12.249486923217773, 26.032630920410156, 0.9656485915184021], step: 8500, lr: 0.00019523244193554745
2024-01-28 19:38:57,662	44k	INFO	====> Epoch: 194, cost 83.77 s
2024-01-28 19:40:20,713	44k	INFO	====> Epoch: 195, cost 83.05 s
2024-01-28 19:41:43,332	44k	INFO	====> Epoch: 196, cost 82.62 s
2024-01-28 19:43:06,635	44k	INFO	====> Epoch: 197, cost 83.30 s
2024-01-28 19:44:29,779	44k	INFO	====> Epoch: 198, cost 83.14 s
2024-01-28 19:45:48,528	44k	INFO	Train Epoch: 199 [86%]
2024-01-28 19:45:48,529	44k	INFO	Losses: [2.2902400493621826, 2.608625888824463, 11.165962219238281, 25.980304718017578, 1.0946660041809082], step: 8750, lr: 0.00019511045216059385
2024-01-28 19:45:52,992	44k	INFO	====> Epoch: 199, cost 83.21 s
2024-01-28 19:47:16,153	44k	INFO	====> Epoch: 200, cost 83.16 s
2024-01-28 19:48:39,405	44k	INFO	====> Epoch: 201, cost 83.25 s
2024-01-28 19:50:02,382	44k	INFO	====> Epoch: 202, cost 82.98 s
2024-01-28 19:51:25,656	44k	INFO	====> Epoch: 203, cost 83.27 s
2024-01-28 19:52:48,839	44k	INFO	====> Epoch: 204, cost 83.18 s
2024-01-28 19:53:58,152	44k	INFO	Train Epoch: 205 [55%]
2024-01-28 19:53:58,153	44k	INFO	Losses: [2.290100574493408, 2.678645610809326, 12.083027839660645, 24.77573013305664, 0.9470669031143188], step: 9000, lr: 0.00019496416504286482
2024-01-28 19:54:11,126	44k	INFO	Saving model and optimizer state at iteration 205 to ./logs/44k/G_9000.pth
2024-01-28 19:54:12,388	44k	INFO	Saving model and optimizer state at iteration 205 to ./logs/44k/D_9000.pth
2024-01-28 19:54:26,433	44k	INFO	====> Epoch: 205, cost 97.59 s
2024-01-28 19:55:52,598	44k	INFO	====> Epoch: 206, cost 86.17 s
2024-01-28 19:57:15,403	44k	INFO	====> Epoch: 207, cost 82.81 s
2024-01-28 19:58:49,436	44k	INFO	====> Epoch: 208, cost 94.03 s
2024-01-28 20:00:12,834	44k	INFO	====> Epoch: 209, cost 83.40 s
2024-01-28 20:01:36,420	44k	INFO	====> Epoch: 210, cost 83.59 s
2024-01-28 20:02:36,536	44k	INFO	Train Epoch: 211 [23%]
2024-01-28 20:02:36,536	44k	INFO	Losses: [2.3041014671325684, 2.638836622238159, 10.126153945922852, 24.296384811401367, 0.8602473735809326], step: 9250, lr: 0.0001948179876061937
2024-01-28 20:03:00,129	44k	INFO	====> Epoch: 211, cost 83.71 s
2024-01-28 20:04:23,333	44k	INFO	====> Epoch: 212, cost 83.20 s
2024-01-28 20:05:46,199	44k	INFO	====> Epoch: 213, cost 82.87 s
2024-01-28 20:07:09,415	44k	INFO	====> Epoch: 214, cost 83.22 s
2024-01-28 20:08:32,583	44k	INFO	====> Epoch: 215, cost 83.17 s
2024-01-28 20:09:52,924	44k	INFO	Train Epoch: 216 [91%]
2024-01-28 20:09:52,925	44k	INFO	Losses: [2.3074886798858643, 2.8066465854644775, 11.070459365844727, 24.873844146728516, 1.1397372484207153], step: 9500, lr: 0.00019469625680044555
2024-01-28 20:09:56,104	44k	INFO	====> Epoch: 216, cost 83.52 s
2024-01-28 20:11:18,721	44k	INFO	====> Epoch: 217, cost 82.62 s
2024-01-28 20:12:42,512	44k	INFO	====> Epoch: 218, cost 83.79 s
2024-01-28 20:14:05,710	44k	INFO	====> Epoch: 219, cost 83.20 s
2024-01-28 20:15:29,125	44k	INFO	====> Epoch: 220, cost 83.41 s
2024-01-28 20:16:52,666	44k	INFO	====> Epoch: 221, cost 83.54 s
2024-01-28 20:18:03,633	44k	INFO	Train Epoch: 222 [59%]
2024-01-28 20:18:03,633	44k	INFO	Losses: [2.20092511177063, 2.6282193660736084, 12.684601783752441, 24.655902862548828, 0.9895181655883789], step: 9750, lr: 0.00019455028023217577
2024-01-28 20:18:16,380	44k	INFO	====> Epoch: 222, cost 83.71 s
2024-01-28 20:19:39,678	44k	INFO	====> Epoch: 223, cost 83.30 s
2024-01-28 20:21:03,881	44k	INFO	====> Epoch: 224, cost 84.20 s
2024-01-28 20:22:26,906	44k	INFO	====> Epoch: 225, cost 83.02 s
2024-01-28 20:23:49,912	44k	INFO	====> Epoch: 226, cost 83.01 s
2024-01-28 20:25:13,323	44k	INFO	====> Epoch: 227, cost 83.41 s
2024-01-28 20:26:15,335	44k	INFO	Train Epoch: 228 [27%]
2024-01-28 20:26:15,336	44k	INFO	Losses: [2.317232608795166, 2.6282408237457275, 11.292990684509277, 24.651479721069336, 0.9943001866340637], step: 10000, lr: 0.00019440441311212466
2024-01-28 20:26:28,056	44k	INFO	Saving model and optimizer state at iteration 228 to ./logs/44k/G_10000.pth
2024-01-28 20:26:29,326	44k	INFO	Saving model and optimizer state at iteration 228 to ./logs/44k/D_10000.pth
2024-01-28 20:26:51,469	44k	INFO	====> Epoch: 228, cost 98.15 s
2024-01-28 20:28:14,682	44k	INFO	====> Epoch: 229, cost 83.21 s
2024-01-28 20:29:41,301	44k	INFO	====> Epoch: 230, cost 86.62 s
2024-01-28 20:31:04,424	44k	INFO	====> Epoch: 231, cost 83.12 s
2024-01-28 20:32:27,865	44k	INFO	====> Epoch: 232, cost 83.44 s
2024-01-28 20:33:49,828	44k	INFO	Train Epoch: 233 [95%]
2024-01-28 20:33:49,829	44k	INFO	Losses: [2.4980885982513428, 2.4052979946136475, 11.66878890991211, 24.13254165649414, 0.9924531579017639], step: 10250, lr: 0.00019428294072582235
2024-01-28 20:33:51,667	44k	INFO	====> Epoch: 233, cost 83.80 s
2024-01-28 20:35:14,589	44k	INFO	====> Epoch: 234, cost 82.92 s
2024-01-28 20:36:37,226	44k	INFO	====> Epoch: 235, cost 82.64 s
2024-01-28 20:37:59,692	44k	INFO	====> Epoch: 236, cost 82.47 s
2024-01-28 20:39:22,581	44k	INFO	====> Epoch: 237, cost 82.89 s
2024-01-28 20:40:45,318	44k	INFO	====> Epoch: 238, cost 82.74 s
2024-01-28 20:41:58,024	44k	INFO	Train Epoch: 239 [64%]
2024-01-28 20:41:58,025	44k	INFO	Losses: [2.3427927494049072, 2.5027740001678467, 12.213457107543945, 25.182857513427734, 0.7715893983840942], step: 10500, lr: 0.0001941372740477537
2024-01-28 20:42:09,326	44k	INFO	====> Epoch: 239, cost 84.01 s
2024-01-28 20:43:32,415	44k	INFO	====> Epoch: 240, cost 83.09 s
2024-01-28 20:44:54,940	44k	INFO	====> Epoch: 241, cost 82.53 s
2024-01-28 20:46:17,677	44k	INFO	====> Epoch: 242, cost 82.74 s
2024-01-28 20:47:40,476	44k	INFO	====> Epoch: 243, cost 82.80 s
2024-01-28 20:49:03,307	44k	INFO	====> Epoch: 244, cost 82.83 s
2024-01-28 20:50:05,188	44k	INFO	Train Epoch: 245 [32%]
2024-01-28 20:50:05,188	44k	INFO	Losses: [2.44639253616333, 2.3479490280151367, 10.692851066589355, 24.945566177368164, 0.9993496537208557], step: 10750, lr: 0.00019399171658555865
2024-01-28 20:50:25,957	44k	INFO	====> Epoch: 245, cost 82.65 s
2024-01-28 20:51:48,987	44k	INFO	====> Epoch: 246, cost 83.03 s
2024-01-28 20:53:12,425	44k	INFO	====> Epoch: 247, cost 83.44 s
2024-01-28 20:54:35,681	44k	INFO	====> Epoch: 248, cost 83.26 s
2024-01-28 20:55:58,764	44k	INFO	====> Epoch: 249, cost 83.08 s
2024-01-28 20:57:21,890	44k	INFO	====> Epoch: 250, cost 83.13 s
2024-01-28 20:58:14,958	44k	INFO	Train Epoch: 251 [0%]
2024-01-28 20:58:14,959	44k	INFO	Losses: [2.4270548820495605, 2.3443894386291504, 11.247111320495605, 23.797130584716797, 0.9896737933158875], step: 11000, lr: 0.0001938462682573509
2024-01-28 20:58:28,003	44k	INFO	Saving model and optimizer state at iteration 251 to ./logs/44k/G_11000.pth
2024-01-28 20:58:29,253	44k	INFO	Saving model and optimizer state at iteration 251 to ./logs/44k/D_11000.pth
2024-01-28 20:58:59,488	44k	INFO	====> Epoch: 251, cost 97.60 s
2024-01-28 21:00:21,871	44k	INFO	====> Epoch: 252, cost 82.38 s
2024-01-28 21:01:44,688	44k	INFO	====> Epoch: 253, cost 82.82 s
2024-01-28 21:03:07,484	44k	INFO	====> Epoch: 254, cost 82.80 s
2024-01-28 21:04:30,696	44k	INFO	====> Epoch: 255, cost 83.21 s
2024-01-28 21:05:43,579	44k	INFO	Train Epoch: 256 [68%]
2024-01-28 21:05:43,579	44k	INFO	Losses: [2.4802393913269043, 2.4104037284851074, 10.744380950927734, 24.242137908935547, 1.072718620300293], step: 11250, lr: 0.0001937251446243836
2024-01-28 21:05:53,577	44k	INFO	====> Epoch: 256, cost 82.88 s
2024-01-28 21:07:15,868	44k	INFO	====> Epoch: 257, cost 82.29 s
2024-01-28 21:08:39,701	44k	INFO	====> Epoch: 258, cost 83.83 s
2024-01-28 21:10:03,552	44k	INFO	====> Epoch: 259, cost 83.85 s
2024-01-28 21:11:27,237	44k	INFO	====> Epoch: 260, cost 83.69 s
2024-01-28 21:12:50,690	44k	INFO	====> Epoch: 261, cost 83.45 s
2024-01-28 21:13:55,903	44k	INFO	Train Epoch: 262 [36%]
2024-01-28 21:13:55,904	44k	INFO	Losses: [2.3622050285339355, 2.5592432022094727, 11.989884376525879, 24.90155601501465, 0.9798169732093811], step: 11500, lr: 0.00019357989616267934
2024-01-28 21:14:15,714	44k	INFO	====> Epoch: 262, cost 85.02 s
2024-01-28 21:15:38,803	44k	INFO	====> Epoch: 263, cost 83.09 s
2024-01-28 21:17:01,810	44k	INFO	====> Epoch: 264, cost 83.01 s
2024-01-28 21:18:25,186	44k	INFO	====> Epoch: 265, cost 83.38 s
2024-01-28 21:19:48,910	44k	INFO	====> Epoch: 266, cost 83.72 s
2024-01-28 21:21:12,875	44k	INFO	====> Epoch: 267, cost 83.97 s
2024-01-28 21:22:08,659	44k	INFO	Train Epoch: 268 [5%]
2024-01-28 21:22:08,661	44k	INFO	Losses: [2.2391374111175537, 2.7932519912719727, 12.189264297485352, 23.536537170410156, 1.0148437023162842], step: 11750, lr: 0.00019343475660328445
2024-01-28 21:22:38,080	44k	INFO	====> Epoch: 268, cost 85.20 s
2024-01-28 21:24:02,061	44k	INFO	====> Epoch: 269, cost 83.98 s
2024-01-28 21:25:26,368	44k	INFO	====> Epoch: 270, cost 84.31 s
2024-01-28 21:26:50,114	44k	INFO	====> Epoch: 271, cost 83.75 s
2024-01-28 21:28:13,436	44k	INFO	====> Epoch: 272, cost 83.32 s
2024-01-28 21:29:28,694	44k	INFO	Train Epoch: 273 [73%]
2024-01-28 21:29:28,695	44k	INFO	Losses: [2.387498378753662, 2.694411039352417, 10.607278823852539, 24.279001235961914, 0.8553541898727417], step: 12000, lr: 0.0001933138901008103
2024-01-28 21:29:41,435	44k	INFO	Saving model and optimizer state at iteration 273 to ./logs/44k/G_12000.pth
2024-01-28 21:29:42,910	44k	INFO	Saving model and optimizer state at iteration 273 to ./logs/44k/D_12000.pth
2024-01-28 21:29:51,562	44k	INFO	====> Epoch: 273, cost 98.13 s
2024-01-28 21:31:12,874	44k	INFO	====> Epoch: 274, cost 81.31 s
2024-01-28 21:32:35,490	44k	INFO	====> Epoch: 275, cost 82.62 s
2024-01-28 21:33:58,114	44k	INFO	====> Epoch: 276, cost 82.62 s
2024-01-28 21:35:20,834	44k	INFO	====> Epoch: 277, cost 82.72 s
2024-01-28 21:36:43,417	44k	INFO	====> Epoch: 278, cost 82.58 s
2024-01-28 21:37:48,534	44k	INFO	Train Epoch: 279 [41%]
2024-01-28 21:37:48,535	44k	INFO	Losses: [2.2318971157073975, 2.55958890914917, 11.690620422363281, 25.16520118713379, 1.0370012521743774], step: 12250, lr: 0.00019316894998362705
2024-01-28 21:38:07,016	44k	INFO	====> Epoch: 279, cost 83.60 s
2024-01-28 21:39:28,493	44k	INFO	====> Epoch: 280, cost 81.48 s
2024-01-28 21:40:50,662	44k	INFO	====> Epoch: 281, cost 82.17 s
2024-01-28 21:42:13,584	44k	INFO	====> Epoch: 282, cost 82.92 s
2024-01-28 21:43:38,251	44k	INFO	====> Epoch: 283, cost 84.67 s
2024-01-28 21:45:02,131	44k	INFO	====> Epoch: 284, cost 83.88 s
2024-01-28 21:45:57,687	44k	INFO	Train Epoch: 285 [9%]
2024-01-28 21:45:57,688	44k	INFO	Losses: [2.1655240058898926, 2.4390344619750977, 14.088316917419434, 24.012619018554688, 0.997361421585083], step: 12500, lr: 0.00019302411853756697
2024-01-28 21:46:26,113	44k	INFO	====> Epoch: 285, cost 83.98 s
2024-01-28 21:47:47,420	44k	INFO	====> Epoch: 286, cost 81.31 s
2024-01-28 21:49:08,946	44k	INFO	====> Epoch: 287, cost 81.53 s
2024-01-28 21:50:31,037	44k	INFO	====> Epoch: 288, cost 82.09 s
2024-01-28 21:51:53,091	44k	INFO	====> Epoch: 289, cost 82.05 s
2024-01-28 21:53:07,917	44k	INFO	Train Epoch: 290 [77%]
2024-01-28 21:53:07,918	44k	INFO	Losses: [2.4590353965759277, 2.512098550796509, 9.170618057250977, 26.39537811279297, 1.0625948905944824], step: 12750, lr: 0.0001929035086197297
2024-01-28 21:53:15,200	44k	INFO	====> Epoch: 290, cost 82.11 s
2024-01-28 21:54:36,446	44k	INFO	====> Epoch: 291, cost 81.25 s
2024-01-28 21:56:00,317	44k	INFO	====> Epoch: 292, cost 83.87 s
2024-01-28 21:57:23,947	44k	INFO	====> Epoch: 293, cost 83.63 s
2024-01-28 21:58:46,866	44k	INFO	====> Epoch: 294, cost 82.92 s
2024-01-28 22:00:10,151	44k	INFO	====> Epoch: 295, cost 83.28 s
2024-01-28 22:01:20,332	44k	INFO	Train Epoch: 296 [45%]
2024-01-28 22:01:20,334	44k	INFO	Losses: [2.258174419403076, 2.614725112915039, 10.80400276184082, 25.497371673583984, 0.9638596773147583], step: 13000, lr: 0.00019275887619249013
2024-01-28 22:01:35,160	44k	INFO	Saving model and optimizer state at iteration 296 to ./logs/44k/G_13000.pth
2024-01-28 22:01:37,064	44k	INFO	Saving model and optimizer state at iteration 296 to ./logs/44k/D_13000.pth
2024-01-28 22:01:53,928	44k	INFO	====> Epoch: 296, cost 103.78 s
2024-01-28 22:03:17,668	44k	INFO	====> Epoch: 297, cost 83.74 s
2024-01-28 22:04:41,104	44k	INFO	====> Epoch: 298, cost 83.44 s
2024-01-28 22:06:05,212	44k	INFO	====> Epoch: 299, cost 84.11 s
2024-01-28 22:07:29,971	44k	INFO	====> Epoch: 300, cost 84.76 s
2024-01-28 22:08:54,269	44k	INFO	====> Epoch: 301, cost 84.30 s
2024-01-28 22:09:52,096	44k	INFO	Train Epoch: 302 [14%]
2024-01-28 22:09:52,097	44k	INFO	Losses: [2.100666046142578, 2.565939426422119, 13.810110092163086, 24.863391876220703, 0.8304051756858826], step: 13250, lr: 0.0001926143522056784
2024-01-28 22:10:18,537	44k	INFO	====> Epoch: 302, cost 84.27 s
2024-01-28 22:11:42,197	44k	INFO	====> Epoch: 303, cost 83.66 s
2024-01-28 22:13:06,188	44k	INFO	====> Epoch: 304, cost 83.99 s
2024-01-28 22:14:30,387	44k	INFO	====> Epoch: 305, cost 84.20 s
2024-01-28 22:15:54,294	44k	INFO	====> Epoch: 306, cost 83.91 s
2024-01-28 22:17:12,511	44k	INFO	Train Epoch: 307 [82%]
2024-01-28 22:17:12,512	44k	INFO	Losses: [2.130643844604492, 2.578598976135254, 11.925558090209961, 24.71491813659668, 1.1077892780303955], step: 13500, lr: 0.0001924939983277806
2024-01-28 22:17:18,508	44k	INFO	====> Epoch: 307, cost 84.21 s
2024-01-28 22:18:43,085	44k	INFO	====> Epoch: 308, cost 84.58 s
2024-01-28 22:20:07,097	44k	INFO	====> Epoch: 309, cost 84.01 s
2024-01-28 22:21:30,847	44k	INFO	====> Epoch: 310, cost 83.75 s
2024-01-28 22:22:54,907	44k	INFO	====> Epoch: 311, cost 84.06 s
2024-01-28 22:24:18,672	44k	INFO	====> Epoch: 312, cost 83.76 s
2024-01-28 22:25:27,388	44k	INFO	Train Epoch: 313 [50%]
2024-01-28 22:25:27,389	44k	INFO	Losses: [2.358771562576294, 2.615790843963623, 11.198365211486816, 24.685693740844727, 0.9150694608688354], step: 13750, lr: 0.00019234967293729698
2024-01-28 22:25:42,865	44k	INFO	====> Epoch: 313, cost 84.19 s
2024-01-28 22:27:07,304	44k	INFO	====> Epoch: 314, cost 84.44 s
2024-01-28 22:28:32,081	44k	INFO	====> Epoch: 315, cost 84.78 s
2024-01-28 22:29:56,499	44k	INFO	====> Epoch: 316, cost 84.42 s
2024-01-28 22:31:20,085	44k	INFO	====> Epoch: 317, cost 83.59 s
2024-01-28 22:32:43,952	44k	INFO	====> Epoch: 318, cost 83.87 s
2024-01-28 22:33:43,380	44k	INFO	Train Epoch: 319 [18%]
2024-01-28 22:33:43,380	44k	INFO	Losses: [2.1211259365081787, 3.195225715637207, 12.905557632446289, 24.64249038696289, 0.7879164814949036], step: 14000, lr: 0.0001922054557570356
2024-01-28 22:33:56,013	44k	INFO	Saving model and optimizer state at iteration 319 to ./logs/44k/G_14000.pth
2024-01-28 22:33:57,272	44k	INFO	Saving model and optimizer state at iteration 319 to ./logs/44k/D_14000.pth
2024-01-28 22:34:22,416	44k	INFO	====> Epoch: 319, cost 98.46 s
2024-01-28 22:35:46,926	44k	INFO	====> Epoch: 320, cost 84.51 s
2024-01-28 22:37:11,225	44k	INFO	====> Epoch: 321, cost 84.30 s
2024-01-28 22:38:35,705	44k	INFO	====> Epoch: 322, cost 84.48 s
2024-01-28 22:40:00,426	44k	INFO	====> Epoch: 323, cost 84.72 s
2024-01-28 22:41:20,562	44k	INFO	Train Epoch: 324 [86%]
2024-01-28 22:41:20,563	44k	INFO	Losses: [2.155156373977661, 2.7456576824188232, 12.12013053894043, 24.39792251586914, 0.9611298441886902], step: 14250, lr: 0.00019208535737553613
2024-01-28 22:41:25,095	44k	INFO	====> Epoch: 324, cost 84.67 s
2024-01-28 22:42:50,824	44k	INFO	====> Epoch: 325, cost 85.73 s
2024-01-28 22:44:15,687	44k	INFO	====> Epoch: 326, cost 84.86 s
2024-01-28 22:45:39,918	44k	INFO	====> Epoch: 327, cost 84.23 s
2024-01-28 22:47:03,061	44k	INFO	====> Epoch: 328, cost 83.14 s
2024-01-28 22:48:28,105	44k	INFO	====> Epoch: 329, cost 85.04 s
2024-01-28 22:49:38,466	44k	INFO	Train Epoch: 330 [55%]
2024-01-28 22:49:38,467	44k	INFO	Losses: [2.2634973526000977, 2.639704704284668, 11.782951354980469, 24.414321899414062, 0.8018622398376465], step: 14500, lr: 0.0001919413383700074
2024-01-28 22:49:52,663	44k	INFO	====> Epoch: 330, cost 84.56 s
2024-01-28 22:51:17,080	44k	INFO	====> Epoch: 331, cost 84.42 s
2024-01-28 22:52:41,401	44k	INFO	====> Epoch: 332, cost 84.32 s
2024-01-28 22:54:06,172	44k	INFO	====> Epoch: 333, cost 84.77 s
2024-01-28 22:55:33,804	44k	INFO	====> Epoch: 334, cost 87.63 s
2024-01-28 22:56:58,080	44k	INFO	====> Epoch: 335, cost 84.28 s
2024-01-28 22:57:58,670	44k	INFO	Train Epoch: 336 [23%]
2024-01-28 22:57:58,672	44k	INFO	Losses: [2.2451906204223633, 2.936521291732788, 11.844206809997559, 25.044721603393555, 0.9485090970993042], step: 14750, lr: 0.00019179742734498406
2024-01-28 22:58:22,298	44k	INFO	====> Epoch: 336, cost 84.22 s
2024-01-28 22:59:47,475	44k	INFO	====> Epoch: 337, cost 85.18 s
2024-01-28 23:01:11,799	44k	INFO	====> Epoch: 338, cost 84.32 s
2024-01-28 23:02:36,302	44k	INFO	====> Epoch: 339, cost 84.50 s
2024-01-28 23:04:00,112	44k	INFO	====> Epoch: 340, cost 83.81 s
2024-01-28 23:05:21,686	44k	INFO	Train Epoch: 341 [91%]
2024-01-28 23:05:21,687	44k	INFO	Losses: [2.2944467067718506, 2.946613073348999, 11.250886917114258, 23.104175567626953, 1.0331141948699951], step: 15000, lr: 0.00019167758391749564
2024-01-28 23:05:34,406	44k	INFO	Saving model and optimizer state at iteration 341 to ./logs/44k/G_15000.pth
2024-01-28 23:05:35,807	44k	INFO	Saving model and optimizer state at iteration 341 to ./logs/44k/D_15000.pth
2024-01-28 23:05:39,095	44k	INFO	====> Epoch: 341, cost 98.98 s
2024-01-28 23:07:02,719	44k	INFO	====> Epoch: 342, cost 83.62 s
2024-01-28 23:08:26,985	44k	INFO	====> Epoch: 343, cost 84.27 s
2024-01-28 23:10:01,910	44k	INFO	====> Epoch: 344, cost 94.92 s
2024-01-28 23:11:25,781	44k	INFO	====> Epoch: 345, cost 83.87 s
2024-01-28 23:12:49,804	44k	INFO	====> Epoch: 346, cost 84.02 s
2024-01-28 23:14:02,269	44k	INFO	Train Epoch: 347 [59%]
2024-01-28 23:14:02,270	44k	INFO	Losses: [2.15079927444458, 2.7763001918792725, 11.663715362548828, 25.016368865966797, 1.1068365573883057], step: 15250, lr: 0.0001915338706465045
2024-01-28 23:14:15,091	44k	INFO	====> Epoch: 347, cost 85.29 s
2024-01-28 23:15:40,396	44k	INFO	====> Epoch: 348, cost 85.30 s
2024-01-28 23:17:05,492	44k	INFO	====> Epoch: 349, cost 85.10 s
2024-01-28 23:18:30,418	44k	INFO	====> Epoch: 350, cost 84.93 s
2024-01-28 23:19:52,365	44k	INFO	====> Epoch: 351, cost 81.95 s
2024-01-28 23:21:13,662	44k	INFO	====> Epoch: 352, cost 81.30 s
2024-01-28 23:22:13,416	44k	INFO	Train Epoch: 353 [27%]
2024-01-28 23:22:13,417	44k	INFO	Losses: [2.2695488929748535, 2.521651268005371, 10.851338386535645, 23.971370697021484, 0.8675779104232788], step: 15500, lr: 0.00019139026512678941
2024-01-28 23:22:35,880	44k	INFO	====> Epoch: 353, cost 82.22 s
2024-01-28 23:23:57,001	44k	INFO	====> Epoch: 354, cost 81.12 s
2024-01-28 23:25:23,639	44k	INFO	====> Epoch: 355, cost 86.64 s
2024-01-28 23:26:46,734	44k	INFO	====> Epoch: 356, cost 83.09 s
2024-01-28 23:28:09,368	44k	INFO	====> Epoch: 357, cost 82.63 s
2024-01-28 23:29:32,981	44k	INFO	Train Epoch: 358 [95%]
2024-01-28 23:29:32,983	44k	INFO	Losses: [2.1648013591766357, 2.7162933349609375, 11.258418083190918, 23.924776077270508, 0.905302107334137], step: 15750, lr: 0.0001912706761120762
2024-01-28 23:29:34,891	44k	INFO	====> Epoch: 358, cost 85.52 s
2024-01-28 23:30:59,348	44k	INFO	====> Epoch: 359, cost 84.46 s
2024-01-28 23:32:22,237	44k	INFO	====> Epoch: 360, cost 82.89 s
2024-01-28 23:33:45,526	44k	INFO	====> Epoch: 361, cost 83.29 s
2024-01-28 23:35:08,225	44k	INFO	====> Epoch: 362, cost 82.70 s
2024-01-28 23:36:31,385	44k	INFO	====> Epoch: 363, cost 83.16 s
2024-01-28 23:37:44,650	44k	INFO	Train Epoch: 364 [64%]
2024-01-28 23:37:44,651	44k	INFO	Losses: [2.2489206790924072, 2.6823925971984863, 13.106971740722656, 25.50105094909668, 0.9368653297424316], step: 16000, lr: 0.00019112726792658598
2024-01-28 23:37:56,954	44k	INFO	Saving model and optimizer state at iteration 364 to ./logs/44k/G_16000.pth
2024-01-28 23:37:58,219	44k	INFO	Saving model and optimizer state at iteration 364 to ./logs/44k/D_16000.pth
2024-01-28 23:38:09,688	44k	INFO	====> Epoch: 364, cost 98.30 s
2024-01-29 23:52:56,119	44k	INFO	{'train': {'log_interval': 250, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0003, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 18, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'yousa': 0}, 'model_dir': './logs/44k'}
2024-01-29 23:52:56,119	44k	WARNING	/root/autodl-tmp/so-vits-svc4 is not a git repository, therefore hash value comparison will be ignored.
2024-01-29 23:52:58,615	44k	INFO	Loaded checkpoint './logs/44k/G_16000.pth' (iteration 364)
2024-01-29 23:52:58,885	44k	INFO	Loaded checkpoint './logs/44k/D_16000.pth' (iteration 364)
2024-01-29 23:54:16,773	44k	INFO	Train Epoch: 364 [64%]
2024-01-29 23:54:16,774	44k	INFO	Losses: [1.9793423414230347, 2.790611982345581, 12.61141586303711, 25.315677642822266, 0.9802707433700562], step: 16000, lr: 0.00019110337701809513
2024-01-29 23:54:30,249	44k	INFO	Saving model and optimizer state at iteration 364 to ./logs/44k/G_16000.pth
2024-01-29 23:54:31,723	44k	INFO	Saving model and optimizer state at iteration 364 to ./logs/44k/D_16000.pth
2024-01-29 23:54:43,143	44k	INFO	====> Epoch: 364, cost 107.03 s
2024-01-29 23:56:07,564	44k	INFO	====> Epoch: 365, cost 84.42 s
2024-01-29 23:57:29,928	44k	INFO	====> Epoch: 366, cost 82.36 s
2024-01-29 23:58:52,432	44k	INFO	====> Epoch: 367, cost 82.50 s
2024-01-30 00:00:16,097	44k	INFO	====> Epoch: 368, cost 83.66 s
2024-01-30 00:01:39,506	44k	INFO	====> Epoch: 369, cost 83.41 s
2024-01-30 00:02:42,231	44k	INFO	Train Epoch: 370 [32%]
2024-01-30 00:02:42,232	44k	INFO	Losses: [2.326199769973755, 2.5187270641326904, 10.902057647705078, 23.825780868530273, 0.9836902618408203], step: 16250, lr: 0.00019096009426772123
2024-01-30 00:03:03,147	44k	INFO	====> Epoch: 370, cost 83.64 s
2024-01-30 00:04:27,227	44k	INFO	====> Epoch: 371, cost 84.08 s
2024-01-30 00:05:50,623	44k	INFO	====> Epoch: 372, cost 83.40 s
2024-01-30 00:07:13,966	44k	INFO	====> Epoch: 373, cost 83.34 s
2024-01-30 00:08:37,285	44k	INFO	====> Epoch: 374, cost 83.32 s
2024-01-30 00:10:00,937	44k	INFO	====> Epoch: 375, cost 83.65 s
2024-01-30 00:10:54,055	44k	INFO	Train Epoch: 376 [0%]
2024-01-30 00:10:54,056	44k	INFO	Losses: [2.315049648284912, 2.538235664367676, 10.718374252319336, 24.515687942504883, 1.1771489381790161], step: 16500, lr: 0.00019081691894583383
2024-01-30 00:11:25,268	44k	INFO	====> Epoch: 376, cost 84.33 s
2024-01-30 00:12:48,310	44k	INFO	====> Epoch: 377, cost 83.04 s
2024-01-30 00:14:11,797	44k	INFO	====> Epoch: 378, cost 83.49 s
2024-01-30 00:15:34,251	44k	INFO	====> Epoch: 379, cost 82.45 s
2024-01-30 00:16:56,383	44k	INFO	====> Epoch: 380, cost 82.13 s
2024-01-30 00:18:09,112	44k	INFO	Train Epoch: 381 [68%]
2024-01-30 00:18:09,112	44k	INFO	Losses: [2.1943602561950684, 2.7213730812072754, 12.321574211120605, 24.185436248779297, 0.9655569791793823], step: 16750, lr: 0.00019069768818290954
2024-01-30 00:18:19,029	44k	INFO	====> Epoch: 381, cost 82.65 s
2024-01-30 00:19:41,573	44k	INFO	====> Epoch: 382, cost 82.54 s
2024-01-30 00:21:04,652	44k	INFO	====> Epoch: 383, cost 83.08 s
2024-01-30 00:22:28,128	44k	INFO	====> Epoch: 384, cost 83.48 s
2024-01-30 00:23:51,022	44k	INFO	====> Epoch: 385, cost 82.89 s
2024-01-30 00:25:14,312	44k	INFO	====> Epoch: 386, cost 83.29 s
2024-01-30 00:26:18,556	44k	INFO	Train Epoch: 387 [36%]
2024-01-30 00:26:18,556	44k	INFO	Losses: [2.0902161598205566, 2.7660255432128906, 13.041029930114746, 23.977067947387695, 0.966688871383667], step: 17000, lr: 0.00019055470960409457
2024-01-30 00:26:31,013	44k	INFO	Saving model and optimizer state at iteration 387 to ./logs/44k/G_17000.pth
2024-01-30 00:26:32,298	44k	INFO	Saving model and optimizer state at iteration 387 to ./logs/44k/D_17000.pth
2024-01-30 00:26:51,827	44k	INFO	====> Epoch: 387, cost 97.51 s
2024-01-30 00:28:15,765	44k	INFO	====> Epoch: 388, cost 83.94 s
2024-01-30 00:29:38,936	44k	INFO	====> Epoch: 389, cost 83.17 s
2024-01-30 00:31:01,433	44k	INFO	====> Epoch: 390, cost 82.50 s
2024-01-30 00:32:24,709	44k	INFO	====> Epoch: 391, cost 83.28 s
2024-01-30 00:33:47,399	44k	INFO	====> Epoch: 392, cost 82.69 s
2024-01-30 08:51:47,713	44k	INFO	{'train': {'log_interval': 250, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0003, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 18, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'yousa': 0}, 'model_dir': './logs/44k'}
2024-01-30 08:51:47,713	44k	WARNING	/root/autodl-tmp/so-vits-svc4 is not a git repository, therefore hash value comparison will be ignored.
2024-01-30 08:51:50,221	44k	INFO	Loaded checkpoint './logs/44k/G_17000.pth' (iteration 387)
2024-01-30 08:51:50,460	44k	INFO	Loaded checkpoint './logs/44k/D_17000.pth' (iteration 387)
2024-01-30 08:52:57,109	44k	INFO	Train Epoch: 387 [36%]
2024-01-30 08:52:57,109	44k	INFO	Losses: [2.4641408920288086, 2.6268796920776367, 10.385153770446777, 24.14996337890625, 0.6535508036613464], step: 17000, lr: 0.00019053089026539405
2024-01-30 08:53:10,413	44k	INFO	Saving model and optimizer state at iteration 387 to ./logs/44k/G_17000.pth
2024-01-30 08:53:11,702	44k	INFO	Saving model and optimizer state at iteration 387 to ./logs/44k/D_17000.pth
2024-01-30 08:53:32,356	44k	INFO	====> Epoch: 387, cost 104.64 s
2024-01-30 08:54:54,323	44k	INFO	====> Epoch: 388, cost 81.97 s
2024-01-30 08:56:16,564	44k	INFO	====> Epoch: 389, cost 82.24 s
2024-01-30 08:57:38,756	44k	INFO	====> Epoch: 390, cost 82.19 s
2024-01-30 08:59:01,067	44k	INFO	====> Epoch: 391, cost 82.31 s
2024-01-30 09:00:22,476	44k	INFO	====> Epoch: 392, cost 81.41 s
2024-01-30 09:01:16,495	44k	INFO	Train Epoch: 393 [5%]
2024-01-30 09:01:16,496	44k	INFO	Losses: [2.2663707733154297, 2.469717502593994, 11.93035888671875, 23.902488708496094, 0.8968271613121033], step: 17250, lr: 0.00019038803674593043
2024-01-30 09:01:45,340	44k	INFO	====> Epoch: 393, cost 82.86 s
2024-01-30 09:03:07,773	44k	INFO	====> Epoch: 394, cost 82.43 s
2024-01-30 09:04:29,768	44k	INFO	====> Epoch: 395, cost 82.00 s
2024-01-30 09:05:51,883	44k	INFO	====> Epoch: 396, cost 82.11 s
2024-01-30 09:07:14,152	44k	INFO	====> Epoch: 397, cost 82.27 s
2024-01-30 09:08:28,812	44k	INFO	Train Epoch: 398 [73%]
2024-01-30 09:08:28,813	44k	INFO	Losses: [2.1416797637939453, 2.9182519912719727, 11.328791618347168, 23.866846084594727, 1.09408700466156], step: 17500, lr: 0.00019026907396737667
2024-01-30 09:08:37,396	44k	INFO	====> Epoch: 398, cost 83.24 s
2024-01-30 09:09:58,820	44k	INFO	====> Epoch: 399, cost 81.42 s
2024-01-30 09:11:21,066	44k	INFO	====> Epoch: 400, cost 82.25 s
2024-01-30 09:12:42,679	44k	INFO	====> Epoch: 401, cost 81.61 s
2024-01-30 09:14:04,954	44k	INFO	====> Epoch: 402, cost 82.27 s
2024-01-30 09:15:27,558	44k	INFO	====> Epoch: 403, cost 82.60 s
2024-01-30 09:16:32,431	44k	INFO	Train Epoch: 404 [41%]
2024-01-30 09:16:32,432	44k	INFO	Losses: [2.235175609588623, 2.7005133628845215, 11.347345352172852, 25.447925567626953, 0.8927918076515198], step: 17750, lr: 0.00019012641674878364
2024-01-30 09:16:50,436	44k	INFO	====> Epoch: 404, cost 82.88 s
2024-01-30 09:18:13,402	44k	INFO	====> Epoch: 405, cost 82.97 s
2024-01-30 09:19:35,651	44k	INFO	====> Epoch: 406, cost 82.25 s
2024-01-30 09:20:57,248	44k	INFO	====> Epoch: 407, cost 81.60 s
2024-01-30 09:22:19,443	44k	INFO	====> Epoch: 408, cost 82.20 s
2024-01-30 09:23:41,213	44k	INFO	====> Epoch: 409, cost 81.77 s
2024-01-30 09:24:35,812	44k	INFO	Train Epoch: 410 [9%]
2024-01-30 09:24:35,812	44k	INFO	Losses: [2.017221689224243, 2.7116858959198, 12.782444953918457, 24.490478515625, 0.9651840329170227], step: 18000, lr: 0.0001899838664896748
2024-01-30 09:24:48,210	44k	INFO	Saving model and optimizer state at iteration 410 to ./logs/44k/G_18000.pth
2024-01-30 09:24:49,542	44k	INFO	Saving model and optimizer state at iteration 410 to ./logs/44k/D_18000.pth
2024-01-30 09:25:16,900	44k	INFO	====> Epoch: 410, cost 95.69 s
2024-01-30 09:26:39,362	44k	INFO	====> Epoch: 411, cost 82.46 s
2024-01-30 09:28:01,098	44k	INFO	====> Epoch: 412, cost 81.74 s
2024-01-30 09:29:22,793	44k	INFO	====> Epoch: 413, cost 81.70 s
2024-01-30 09:30:44,704	44k	INFO	====> Epoch: 414, cost 81.91 s
2024-01-30 09:32:00,198	44k	INFO	Train Epoch: 415 [77%]
2024-01-30 09:32:00,199	44k	INFO	Losses: [2.3276004791259766, 2.3424525260925293, 11.559799194335938, 23.73872947692871, 1.0743863582611084], step: 18250, lr: 0.00018986515625438747
2024-01-30 09:32:07,371	44k	INFO	====> Epoch: 415, cost 82.67 s
2024-01-30 09:33:29,436	44k	INFO	====> Epoch: 416, cost 82.07 s
2024-01-30 09:34:52,822	44k	INFO	====> Epoch: 417, cost 83.39 s
2024-01-30 09:36:15,938	44k	INFO	====> Epoch: 418, cost 83.12 s
2024-01-30 09:37:37,307	44k	INFO	====> Epoch: 419, cost 81.37 s
2024-01-30 09:38:59,318	44k	INFO	====> Epoch: 420, cost 82.01 s
2024-01-30 09:40:05,522	44k	INFO	Train Epoch: 421 [45%]
2024-01-30 09:40:05,523	44k	INFO	Losses: [2.4127378463745117, 2.6785378456115723, 10.023641586303711, 23.83014678955078, 0.7193333506584167], step: 18500, lr: 0.00018972280187942673
2024-01-30 09:40:22,106	44k	INFO	====> Epoch: 421, cost 82.79 s
2024-01-30 09:41:44,035	44k	INFO	====> Epoch: 422, cost 81.93 s
2024-01-30 09:43:05,378	44k	INFO	====> Epoch: 423, cost 81.34 s
2024-01-30 09:44:26,840	44k	INFO	====> Epoch: 424, cost 81.46 s
2024-01-30 09:45:48,745	44k	INFO	====> Epoch: 425, cost 81.91 s
2024-01-30 09:47:10,874	44k	INFO	====> Epoch: 426, cost 82.13 s
2024-01-30 09:48:07,346	44k	INFO	Train Epoch: 427 [14%]
2024-01-30 09:48:07,347	44k	INFO	Losses: [2.0860350131988525, 2.6676220893859863, 13.517799377441406, 26.162599563598633, 0.8120923638343811], step: 18750, lr: 0.00018958055423688846
2024-01-30 09:48:33,510	44k	INFO	====> Epoch: 427, cost 82.64 s
2024-01-30 09:49:56,234	44k	INFO	====> Epoch: 428, cost 82.72 s
2024-01-30 09:51:18,008	44k	INFO	====> Epoch: 429, cost 81.77 s
2024-01-30 09:52:43,240	44k	INFO	====> Epoch: 430, cost 85.23 s
2024-01-30 09:54:04,872	44k	INFO	====> Epoch: 431, cost 81.63 s
2024-01-30 09:55:21,249	44k	INFO	Train Epoch: 432 [82%]
2024-01-30 09:55:21,249	44k	INFO	Losses: [2.1680312156677246, 2.7304606437683105, 13.065619468688965, 24.367319107055664, 0.9777384996414185], step: 19000, lr: 0.00018946209600874948
2024-01-30 09:55:33,407	44k	INFO	Saving model and optimizer state at iteration 432 to ./logs/44k/G_19000.pth
2024-01-30 09:55:34,625	44k	INFO	Saving model and optimizer state at iteration 432 to ./logs/44k/D_19000.pth
2024-01-30 09:55:40,424	44k	INFO	====> Epoch: 432, cost 95.55 s
2024-01-30 09:57:02,042	44k	INFO	====> Epoch: 433, cost 81.62 s
2024-01-30 09:58:23,823	44k	INFO	====> Epoch: 434, cost 81.78 s
2024-01-30 09:59:45,402	44k	INFO	====> Epoch: 435, cost 81.58 s
2024-01-30 10:01:06,912	44k	INFO	====> Epoch: 436, cost 81.51 s
2024-01-30 10:02:28,964	44k	INFO	====> Epoch: 437, cost 82.05 s
2024-01-30 10:03:36,275	44k	INFO	Train Epoch: 438 [50%]
2024-01-30 10:03:36,275	44k	INFO	Losses: [2.306784152984619, 2.6737236976623535, 11.409200668334961, 23.617752075195312, 0.7670560479164124], step: 19250, lr: 0.00018932004383452147
2024-01-30 10:03:51,679	44k	INFO	====> Epoch: 438, cost 82.72 s
2024-01-30 10:05:13,267	44k	INFO	====> Epoch: 439, cost 81.59 s
2024-01-30 10:06:34,885	44k	INFO	====> Epoch: 440, cost 81.62 s
2024-01-30 10:07:56,850	44k	INFO	====> Epoch: 441, cost 81.97 s
2024-01-30 10:09:18,652	44k	INFO	====> Epoch: 442, cost 81.80 s
2024-01-30 10:10:40,561	44k	INFO	====> Epoch: 443, cost 81.91 s
2024-01-30 10:11:38,619	44k	INFO	Train Epoch: 444 [18%]
2024-01-30 10:11:38,620	44k	INFO	Losses: [2.439804792404175, 2.5081002712249756, 12.118471145629883, 24.104904174804688, 0.8302078247070312], step: 19500, lr: 0.00018917809816613621
2024-01-30 10:12:03,345	44k	INFO	====> Epoch: 444, cost 82.78 s
2024-01-30 10:13:25,166	44k	INFO	====> Epoch: 445, cost 81.82 s
2024-01-30 10:14:46,876	44k	INFO	====> Epoch: 446, cost 81.71 s
2024-01-30 10:16:08,554	44k	INFO	====> Epoch: 447, cost 81.68 s
2024-01-30 10:17:30,558	44k	INFO	====> Epoch: 448, cost 82.00 s
2024-01-30 10:18:48,505	44k	INFO	Train Epoch: 449 [86%]
2024-01-30 10:18:48,507	44k	INFO	Losses: [2.382728099822998, 2.6432247161865234, 12.043296813964844, 24.13397979736328, 0.9632593393325806], step: 19750, lr: 0.0001890598914101655
2024-01-30 10:18:52,980	44k	INFO	====> Epoch: 449, cost 82.42 s
2024-01-30 10:20:14,257	44k	INFO	====> Epoch: 450, cost 81.28 s
2024-01-30 10:21:36,185	44k	INFO	====> Epoch: 451, cost 81.93 s
2024-01-30 10:23:00,500	44k	INFO	====> Epoch: 452, cost 84.31 s
2024-01-30 10:24:22,559	44k	INFO	====> Epoch: 453, cost 82.06 s
2024-01-30 10:25:44,944	44k	INFO	====> Epoch: 454, cost 82.39 s
2024-01-30 10:26:53,667	44k	INFO	Train Epoch: 455 [55%]
2024-01-30 10:26:53,668	44k	INFO	Losses: [2.4056315422058105, 2.5200560092926025, 11.368322372436523, 24.43564224243164, 0.8994709849357605], step: 20000, lr: 0.00018891814079513542
2024-01-30 10:27:05,981	44k	INFO	Saving model and optimizer state at iteration 455 to ./logs/44k/G_20000.pth
2024-01-30 10:27:07,346	44k	INFO	Saving model and optimizer state at iteration 455 to ./logs/44k/D_20000.pth
2024-01-30 10:27:21,298	44k	INFO	====> Epoch: 455, cost 96.35 s
2024-02-01 20:26:19,623	44k	INFO	{'train': {'log_interval': 250, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0003, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 18, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'yousa': 0}, 'model_dir': './logs/44k'}
2024-02-01 20:26:19,623	44k	WARNING	/root/autodl-tmp/so-vits-svc4 is not a git repository, therefore hash value comparison will be ignored.
2024-02-01 20:26:23,645	44k	INFO	Loaded checkpoint './logs/44k/G_20000.pth' (iteration 455)
2024-02-01 20:26:24,440	44k	INFO	Loaded checkpoint './logs/44k/D_20000.pth' (iteration 455)
2024-02-01 20:41:39,733	44k	INFO	{'train': {'log_interval': 250, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0003, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 18, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 0}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 200}, 'spk': {'yousa': 0}, 'model_dir': './logs/44k'}
2024-02-01 20:41:39,734	44k	WARNING	/root/autodl-tmp/so-vits-svc4 is not a git repository, therefore hash value comparison will be ignored.
2024-02-01 20:41:43,303	44k	INFO	Loaded checkpoint './logs/44k/G_20000.pth' (iteration 455)
2024-02-01 20:41:43,657	44k	INFO	Loaded checkpoint './logs/44k/D_20000.pth' (iteration 455)
2024-02-01 20:43:01,435	44k	INFO	Train Epoch: 455 [55%]
2024-02-01 20:43:01,436	44k	INFO	Losses: [2.168741226196289, 2.7463202476501465, 12.532279014587402, 23.87228775024414, 1.0357524156570435], step: 20000, lr: 0.000188894526027536
2024-02-01 20:43:14,917	44k	INFO	Saving model and optimizer state at iteration 455 to ./logs/44k/G_20000.pth
2024-02-01 20:43:16,515	44k	INFO	Saving model and optimizer state at iteration 455 to ./logs/44k/D_20000.pth
2024-02-01 20:43:31,098	44k	INFO	====> Epoch: 455, cost 111.37 s
2024-02-01 20:44:57,007	44k	INFO	====> Epoch: 456, cost 85.91 s
2024-02-01 20:46:23,036	44k	INFO	====> Epoch: 457, cost 86.03 s
2024-02-01 20:47:49,494	44k	INFO	====> Epoch: 458, cost 86.46 s
2024-02-01 20:49:15,231	44k	INFO	====> Epoch: 459, cost 85.74 s
2024-02-01 20:50:41,252	44k	INFO	====> Epoch: 460, cost 86.02 s
2024-02-01 20:51:44,070	44k	INFO	Train Epoch: 461 [23%]
2024-02-01 20:51:44,071	44k	INFO	Losses: [2.0643577575683594, 2.7299492359161377, 11.976347923278809, 24.49662208557129, 0.8850858807563782], step: 20250, lr: 0.00018875289939779185
2024-02-01 20:52:09,611	44k	INFO	====> Epoch: 461, cost 88.36 s
2024-02-01 20:53:35,348	44k	INFO	====> Epoch: 462, cost 85.74 s
2024-02-01 20:55:01,111	44k	INFO	====> Epoch: 463, cost 85.76 s
2024-02-01 20:56:27,434	44k	INFO	====> Epoch: 464, cost 86.32 s
2024-02-01 20:57:53,709	44k	INFO	====> Epoch: 465, cost 86.27 s
2024-02-01 20:59:17,285	44k	INFO	Train Epoch: 466 [91%]
2024-02-01 20:59:17,286	44k	INFO	Losses: [2.424739360809326, 2.642615556716919, 9.455389976501465, 23.282821655273438, 0.9241303205490112], step: 20500, lr: 0.00018863495832462237
2024-02-01 20:59:20,594	44k	INFO	====> Epoch: 466, cost 86.88 s
2024-02-01 21:00:46,327	44k	INFO	====> Epoch: 467, cost 85.73 s
2024-02-01 21:02:12,433	44k	INFO	====> Epoch: 468, cost 86.11 s
2024-02-01 21:03:38,534	44k	INFO	====> Epoch: 469, cost 86.10 s
2024-02-01 21:05:04,749	44k	INFO	====> Epoch: 470, cost 86.22 s
2024-02-01 21:06:31,223	44k	INFO	====> Epoch: 471, cost 86.47 s
2024-02-01 21:07:44,988	44k	INFO	Train Epoch: 472 [59%]
2024-02-01 21:07:44,989	44k	INFO	Losses: [2.1460530757904053, 2.9179182052612305, 13.663110733032227, 24.507823944091797, 1.0193201303482056], step: 20750, lr: 0.00018849352630982935
2024-02-01 21:07:57,945	44k	INFO	====> Epoch: 472, cost 86.72 s
2024-02-01 21:09:23,864	44k	INFO	====> Epoch: 473, cost 85.92 s
2024-02-01 21:10:49,880	44k	INFO	====> Epoch: 474, cost 86.02 s
2024-02-01 21:12:16,003	44k	INFO	====> Epoch: 475, cost 86.12 s
2024-02-01 21:13:43,430	44k	INFO	====> Epoch: 476, cost 87.43 s
2024-02-01 21:15:09,569	44k	INFO	====> Epoch: 477, cost 86.14 s
2024-02-01 21:16:13,484	44k	INFO	Train Epoch: 478 [27%]
2024-02-01 21:16:13,485	44k	INFO	Losses: [2.2967655658721924, 2.8367340564727783, 12.65715503692627, 24.0206241607666, 0.9111209511756897], step: 21000, lr: 0.00018835220033590485
2024-02-01 21:16:26,393	44k	INFO	Saving model and optimizer state at iteration 478 to ./logs/44k/G_21000.pth
2024-02-01 21:16:27,959	44k	INFO	Saving model and optimizer state at iteration 478 to ./logs/44k/D_21000.pth
2024-02-01 21:16:50,581	44k	INFO	====> Epoch: 478, cost 101.01 s
2024-02-01 21:18:17,865	44k	INFO	====> Epoch: 479, cost 87.28 s
2024-02-01 21:19:45,707	44k	INFO	====> Epoch: 480, cost 87.84 s
2024-02-01 21:21:14,000	44k	INFO	====> Epoch: 481, cost 88.29 s
2024-02-01 21:22:40,177	44k	INFO	====> Epoch: 482, cost 86.18 s
2024-02-01 21:24:04,622	44k	INFO	Train Epoch: 483 [95%]
2024-02-01 21:24:04,623	44k	INFO	Losses: [2.20444655418396, 2.3960089683532715, 11.843417167663574, 24.44639015197754, 0.9635979533195496], step: 21250, lr: 0.0001882345096370477
2024-02-01 21:24:06,583	44k	INFO	====> Epoch: 483, cost 86.41 s
2024-02-01 21:25:32,815	44k	INFO	====> Epoch: 484, cost 86.23 s
2024-02-01 21:26:58,928	44k	INFO	====> Epoch: 485, cost 86.11 s
2024-02-01 21:28:25,939	44k	INFO	====> Epoch: 486, cost 87.01 s
2024-02-01 21:29:52,089	44k	INFO	====> Epoch: 487, cost 86.15 s
2024-02-01 21:31:18,258	44k	INFO	====> Epoch: 488, cost 86.17 s
2024-02-01 21:32:33,127	44k	INFO	Train Epoch: 489 [64%]
2024-02-01 21:32:33,128	44k	INFO	Losses: [2.1293747425079346, 2.8160955905914307, 11.803021430969238, 24.415035247802734, 0.834871232509613], step: 21500, lr: 0.00018809337786493085
2024-02-01 21:32:44,726	44k	INFO	====> Epoch: 489, cost 86.47 s
2024-02-01 21:34:10,792	44k	INFO	====> Epoch: 490, cost 86.07 s
2024-02-01 21:35:41,244	44k	INFO	====> Epoch: 491, cost 90.45 s
2024-02-01 21:37:06,992	44k	INFO	====> Epoch: 492, cost 85.75 s
2024-02-01 21:38:33,207	44k	INFO	====> Epoch: 493, cost 86.21 s
2024-02-01 21:39:59,113	44k	INFO	====> Epoch: 494, cost 85.91 s
2024-02-01 21:41:04,215	44k	INFO	Train Epoch: 495 [32%]
2024-02-01 21:41:04,216	44k	INFO	Losses: [2.1175897121429443, 2.6600983142852783, 13.073982238769531, 23.752119064331055, 0.9035423994064331], step: 21750, lr: 0.00018795235190857083
2024-02-01 21:41:25,356	44k	INFO	====> Epoch: 495, cost 86.24 s
2024-02-01 21:42:51,063	44k	INFO	====> Epoch: 496, cost 85.71 s
2024-02-01 21:44:16,853	44k	INFO	====> Epoch: 497, cost 85.79 s
2024-02-01 21:45:42,520	44k	INFO	====> Epoch: 498, cost 85.67 s
2024-02-01 21:47:08,175	44k	INFO	====> Epoch: 499, cost 85.66 s
2024-02-01 21:48:34,107	44k	INFO	====> Epoch: 500, cost 85.93 s
2024-02-01 21:49:29,878	44k	INFO	Train Epoch: 501 [0%]
2024-02-01 21:49:29,879	44k	INFO	Losses: [2.256394147872925, 2.643286943435669, 11.858271598815918, 25.24526596069336, 1.0901809930801392], step: 22000, lr: 0.0001878114316886306
2024-02-01 21:49:43,185	44k	INFO	Saving model and optimizer state at iteration 501 to ./logs/44k/G_22000.pth
2024-02-01 21:49:44,620	44k	INFO	Saving model and optimizer state at iteration 501 to ./logs/44k/D_22000.pth
2024-02-01 21:50:15,451	44k	INFO	====> Epoch: 501, cost 101.34 s
2024-02-01 21:51:41,408	44k	INFO	====> Epoch: 502, cost 85.96 s
2024-02-01 21:53:07,243	44k	INFO	====> Epoch: 503, cost 85.83 s
2024-02-01 21:54:32,879	44k	INFO	====> Epoch: 504, cost 85.64 s
2024-02-01 21:55:58,917	44k	INFO	====> Epoch: 505, cost 86.04 s
2024-02-01 21:57:16,062	44k	INFO	Train Epoch: 506 [68%]
2024-02-01 21:57:16,063	44k	INFO	Losses: [2.5999748706817627, 2.1626927852630615, 11.896836280822754, 23.988475799560547, 0.956980288028717], step: 22250, lr: 0.00018769407888569344
2024-02-01 21:57:26,248	44k	INFO	====> Epoch: 506, cost 87.33 s
2024-02-01 21:58:51,936	44k	INFO	====> Epoch: 507, cost 85.69 s
2024-02-01 22:00:17,525	44k	INFO	====> Epoch: 508, cost 85.59 s
2024-02-01 22:01:43,322	44k	INFO	====> Epoch: 509, cost 85.80 s
2024-02-01 22:03:08,922	44k	INFO	====> Epoch: 510, cost 85.60 s
2024-02-01 22:04:34,615	44k	INFO	====> Epoch: 511, cost 85.69 s
2024-02-01 22:05:41,261	44k	INFO	Train Epoch: 512 [36%]
2024-02-01 22:05:41,262	44k	INFO	Losses: [2.24965238571167, 2.7234058380126953, 10.306670188903809, 22.993579864501953, 0.9056100845336914], step: 22500, lr: 0.00018755335230999777
2024-02-01 22:06:00,985	44k	INFO	====> Epoch: 512, cost 86.37 s
2024-02-01 22:07:26,726	44k	INFO	====> Epoch: 513, cost 85.74 s
2024-02-01 22:08:52,414	44k	INFO	====> Epoch: 514, cost 85.69 s
2024-02-01 22:10:17,857	44k	INFO	====> Epoch: 515, cost 85.44 s
2024-02-01 22:11:43,500	44k	INFO	====> Epoch: 516, cost 85.64 s
2024-02-01 22:13:09,164	44k	INFO	====> Epoch: 517, cost 85.66 s
2024-02-01 22:14:05,959	44k	INFO	Train Epoch: 518 [5%]
2024-02-01 22:14:05,960	44k	INFO	Losses: [2.371417284011841, 2.6849775314331055, 10.764078140258789, 23.200950622558594, 0.7778471112251282], step: 22750, lr: 0.00018741273124625658
2024-02-01 22:14:35,331	44k	INFO	====> Epoch: 518, cost 86.17 s
2024-02-01 22:16:00,847	44k	INFO	====> Epoch: 519, cost 85.52 s
2024-02-01 22:17:26,715	44k	INFO	====> Epoch: 520, cost 85.87 s
2024-02-01 22:18:52,513	44k	INFO	====> Epoch: 521, cost 85.80 s
2024-02-01 22:20:18,122	44k	INFO	====> Epoch: 522, cost 85.61 s
2024-02-01 22:21:35,392	44k	INFO	Train Epoch: 523 [73%]
2024-02-01 22:21:35,393	44k	INFO	Losses: [2.108260154724121, 2.995537281036377, 10.757085800170898, 23.302629470825195, 0.9462106227874756], step: 23000, lr: 0.0001872956275688067
2024-02-01 22:21:48,339	44k	INFO	Saving model and optimizer state at iteration 523 to ./logs/44k/G_23000.pth
2024-02-01 22:21:49,742	44k	INFO	Saving model and optimizer state at iteration 523 to ./logs/44k/D_23000.pth
2024-02-01 22:21:58,501	44k	INFO	====> Epoch: 523, cost 100.38 s
2024-02-01 22:23:24,324	44k	INFO	====> Epoch: 524, cost 85.82 s
2024-02-01 22:24:49,897	44k	INFO	====> Epoch: 525, cost 85.57 s
2024-02-01 22:26:15,719	44k	INFO	====> Epoch: 526, cost 85.82 s
2024-02-01 22:27:41,385	44k	INFO	====> Epoch: 527, cost 85.67 s
2024-02-01 22:29:08,106	44k	INFO	====> Epoch: 528, cost 86.72 s
2024-02-01 22:30:16,056	44k	INFO	Train Epoch: 529 [41%]
2024-02-01 22:30:16,057	44k	INFO	Losses: [1.9298075437545776, 2.8745179176330566, 15.232686042785645, 24.085845947265625, 0.8243006467819214], step: 23250, lr: 0.00018715519973822723
2024-02-01 22:30:34,501	44k	INFO	====> Epoch: 529, cost 86.39 s
2024-02-01 22:32:00,349	44k	INFO	====> Epoch: 530, cost 85.85 s
2024-02-01 22:33:26,606	44k	INFO	====> Epoch: 531, cost 86.26 s
2024-02-01 22:34:52,339	44k	INFO	====> Epoch: 532, cost 85.73 s
2024-02-01 22:36:18,068	44k	INFO	====> Epoch: 533, cost 85.73 s
2024-02-01 22:37:43,999	44k	INFO	====> Epoch: 534, cost 85.93 s
2024-02-01 22:38:42,130	44k	INFO	Train Epoch: 535 [9%]
2024-02-01 22:38:42,131	44k	INFO	Losses: [2.2078158855438232, 2.5510194301605225, 12.767921447753906, 24.614337921142578, 0.8822206854820251], step: 23500, lr: 0.00018701487719561334
2024-02-01 22:39:10,190	44k	INFO	====> Epoch: 535, cost 86.19 s
2024-02-01 22:40:36,216	44k	INFO	====> Epoch: 536, cost 86.03 s
2024-02-01 22:42:01,751	44k	INFO	====> Epoch: 537, cost 85.54 s
2024-02-01 22:43:27,685	44k	INFO	====> Epoch: 538, cost 85.93 s
2024-02-01 22:44:53,765	44k	INFO	====> Epoch: 539, cost 86.08 s
2024-02-01 22:46:12,859	44k	INFO	Train Epoch: 540 [77%]
2024-02-01 22:46:12,860	44k	INFO	Losses: [2.0824501514434814, 2.764096975326538, 12.02713680267334, 24.916893005371094, 0.9502236843109131], step: 23750, lr: 0.00018689802211478823
2024-02-01 22:46:20,285	44k	INFO	====> Epoch: 540, cost 86.52 s
2024-02-01 22:47:46,283	44k	INFO	====> Epoch: 541, cost 86.00 s
2024-02-01 22:49:11,903	44k	INFO	====> Epoch: 542, cost 85.62 s
2024-02-01 22:50:38,193	44k	INFO	====> Epoch: 543, cost 86.29 s
2024-02-01 22:52:04,005	44k	INFO	====> Epoch: 544, cost 85.81 s
2024-02-01 22:53:29,844	44k	INFO	====> Epoch: 545, cost 85.84 s
2024-02-01 22:54:39,140	44k	INFO	Train Epoch: 546 [45%]
2024-02-01 22:54:39,141	44k	INFO	Losses: [2.2554142475128174, 2.550410032272339, 11.89471435546875, 24.950265884399414, 0.9486100673675537], step: 24000, lr: 0.00018675789239512602
2024-02-01 22:54:52,140	44k	INFO	Saving model and optimizer state at iteration 546 to ./logs/44k/G_24000.pth
2024-02-01 22:54:53,691	44k	INFO	Saving model and optimizer state at iteration 546 to ./logs/44k/D_24000.pth
2024-02-01 22:55:10,746	44k	INFO	====> Epoch: 546, cost 100.90 s
2024-02-01 22:56:36,601	44k	INFO	====> Epoch: 547, cost 85.85 s
2024-02-01 22:58:02,378	44k	INFO	====> Epoch: 548, cost 85.78 s
2024-02-01 22:59:27,987	44k	INFO	====> Epoch: 549, cost 85.61 s
2024-02-01 23:00:54,000	44k	INFO	====> Epoch: 550, cost 86.01 s
2024-02-01 23:02:19,992	44k	INFO	====> Epoch: 551, cost 85.99 s
2024-02-01 23:03:19,702	44k	INFO	Train Epoch: 552 [14%]
2024-02-01 23:03:19,703	44k	INFO	Losses: [2.013002395629883, 2.763613700866699, 14.014647483825684, 26.436920166015625, 0.7187992334365845], step: 24250, lr: 0.0001866178677399161
2024-02-01 23:03:46,317	44k	INFO	====> Epoch: 552, cost 86.33 s
2024-02-01 23:05:13,617	44k	INFO	====> Epoch: 553, cost 87.30 s
2024-02-01 23:06:39,513	44k	INFO	====> Epoch: 554, cost 85.90 s
2024-02-01 23:08:05,314	44k	INFO	====> Epoch: 555, cost 85.80 s
2024-02-01 23:09:31,177	44k	INFO	====> Epoch: 556, cost 85.86 s
2024-02-01 23:10:52,394	44k	INFO	Train Epoch: 557 [82%]
2024-02-01 23:10:52,395	44k	INFO	Losses: [2.16288423538208, 2.7780368328094482, 11.376089096069336, 24.251571655273438, 1.013298749923706], step: 24500, lr: 0.00018650126072797577
2024-02-01 23:10:58,639	44k	INFO	====> Epoch: 557, cost 87.46 s
2024-02-01 23:12:24,650	44k	INFO	====> Epoch: 558, cost 86.01 s
2024-02-01 23:13:51,033	44k	INFO	====> Epoch: 559, cost 86.38 s
2024-02-01 23:15:17,078	44k	INFO	====> Epoch: 560, cost 86.04 s
2024-02-01 23:16:43,202	44k	INFO	====> Epoch: 561, cost 86.12 s
2024-02-01 23:18:09,238	44k	INFO	====> Epoch: 562, cost 86.04 s
2024-02-01 23:19:20,028	44k	INFO	Train Epoch: 563 [50%]
2024-02-01 23:19:20,029	44k	INFO	Losses: [2.3795173168182373, 2.6259610652923584, 11.109294891357422, 23.062437057495117, 0.836262047290802], step: 24750, lr: 0.00018636142848637817
2024-02-01 23:19:35,709	44k	INFO	====> Epoch: 563, cost 86.47 s
2024-02-01 23:21:01,680	44k	INFO	====> Epoch: 564, cost 85.97 s
2024-02-01 23:22:27,169	44k	INFO	====> Epoch: 565, cost 85.49 s
2024-02-01 23:23:53,160	44k	INFO	====> Epoch: 566, cost 85.99 s
2024-02-01 23:25:18,962	44k	INFO	====> Epoch: 567, cost 85.80 s
2024-02-01 23:26:44,641	44k	INFO	====> Epoch: 568, cost 85.68 s
2024-02-01 23:27:45,875	44k	INFO	Train Epoch: 569 [18%]
2024-02-01 23:27:45,876	44k	INFO	Losses: [2.0098202228546143, 2.8455429077148438, 13.311580657958984, 24.5434513092041, 0.8100630044937134], step: 25000, lr: 0.00018622170108619407
2024-02-01 23:27:59,010	44k	INFO	Saving model and optimizer state at iteration 569 to ./logs/44k/G_25000.pth
2024-02-01 23:28:00,415	44k	INFO	Saving model and optimizer state at iteration 569 to ./logs/44k/D_25000.pth
2024-02-01 23:28:26,057	44k	INFO	====> Epoch: 569, cost 101.42 s
